Medical Retrieval-Augmented Generation (RAG) System

This project is an industry-ready Medical Question Answering system built using Retrieval-Augmented Generation (RAG).
It allows users to ask natural language questions over medical research PDFs and generates grounded, source-attributed answers using a local LLM (Ollama).

ðŸš€ Key Features

ðŸ“„ Medical PDF ingestion

âœ‚ï¸ Text cleaning & chunking

ðŸ§  Semantic embeddings (Sentence Transformers)

ðŸ“¦ FAISS vector database for fast retrieval

ðŸ” Context-aware semantic search

ðŸ¤– Local LLM inference using Ollama (privacy-preserving)

ðŸš« Hallucination control (answers only from documents)

ðŸ“š Page-level source attribution

ðŸ–¥ï¸ Streamlit UI for live demo

ðŸ§  System Architecture (High Level)
User Question
     â†“
Streamlit UI
     â†“
RAG Pipeline
     â†“
Semantic Retriever (FAISS)
     â†“
Relevant Text Chunks
     â†“
Prompt Construction
     â†“
Local LLM (Ollama)
     â†“
Grounded Answer + Sources

ðŸ—ï¸ Detailed Architecture Diagram
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Medical PDFs   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF Loader (PyMuPDF)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Cleaner           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunker (Overlap)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embeddings Generator   â”‚
â”‚ (SentenceTransformers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FAISS Vector Store     â”‚
â”‚ (Text + Metadata)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Semantic Retriever     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Builder         â”‚
â”‚ (Context + Guardrails) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama LLM (Local)     â”‚
â”‚ gemma3:4b              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Answer + Source Pages  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ðŸ“ Project Structure
rag-medical-qa/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                    # Medical PDFs
â”‚
â”œâ”€â”€ data_ingestion/
â”‚   â””â”€â”€ pdf_loader.py            # Extract text from PDFs
â”‚
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ text_cleaner.py          # Clean raw text
â”‚   â””â”€â”€ chunker.py               # Chunk text with overlap
â”‚
â”œâ”€â”€ vector_store/
â”‚   â”œâ”€â”€ embedder.py              # Generate embeddings
â”‚   â””â”€â”€ faiss_store.py           # FAISS index handling
â”‚
â”œâ”€â”€ retriever/
â”‚   â””â”€â”€ semantic_retriever.py    # Semantic search
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ ollama_client.py         # Local LLM interface
â”‚
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ rag_pipeline.py          # End-to-end RAG logic
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                   # Streamlit UI
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_embeddings.py       # Build FAISS index
â”‚   â””â”€â”€ test_rag_llm.py           # Full RAG test
â”‚
â””â”€â”€ README.md

ðŸ§© Module-by-Module Explanation
1ï¸âƒ£ PDF Loader (pdf_loader.py)

Uses PyMuPDF

Extracts text page-by-page

Preserves document name & page number

Why?
Page-level granularity is required for traceable answers.

2ï¸âƒ£ Text Cleaner (text_cleaner.py)

Removes extra whitespace

Normalizes formatting

Keeps medical terminology intact

Why?
Cleaner text improves embedding quality.

3ï¸âƒ£ Chunker (chunker.py)

Splits text into overlapping chunks

Prevents context loss across boundaries

Why?
LLMs perform better on short, coherent chunks.

4ï¸âƒ£ Embedder (embedder.py)

Uses Sentence-Transformers (MiniLM)

Converts text into dense vectors

Why?
Enables semantic search, not keyword search.

5ï¸âƒ£ FAISS Vector Store (faiss_store.py)

Stores embeddings + metadata + actual text

Enables fast similarity search

Why FAISS?

Scales well

Industry standard

CPU-efficient

6ï¸âƒ£ Semantic Retriever (semantic_retriever.py)

Converts query â†’ embedding

Retrieves top-K relevant chunks

Why?
Ensures only relevant medical context is passed to the LLM.

7ï¸âƒ£ RAG Pipeline (rag_pipeline.py)

Orchestrates retrieval + generation

Builds hallucination-safe prompts

Returns answer + sources

Key Design Choice:
LLM sees only retrieved content, nothing else.

8ï¸âƒ£ Ollama Client (ollama_client.py)

Runs local LLM via CLI

UTF-8 safe for Windows

No API keys required

Why Ollama?

Offline

Privacy-preserving

Cost-free

Ideal for medical data

9ï¸âƒ£ Streamlit UI (app.py)

Interactive web interface

Displays answer + sources

Optional context inspection

Why Streamlit?

Fast prototyping

Perfect for interviews & demos

ðŸ” Hallucination Control Strategy

LLM instructed to:

â€œAnswer ONLY using the provided contextâ€

If information is missing â†’ explicit fallback

No external knowledge injection

ðŸ§ª How to Run
1ï¸âƒ£ Build embeddings
python -m tests.test_embeddings

2ï¸âƒ£ Run full RAG test
python -m tests.test_rag_llm

3ï¸âƒ£ Launch UI
streamlit run app/app.py
------>Local URL: http://localhost:8501------->
